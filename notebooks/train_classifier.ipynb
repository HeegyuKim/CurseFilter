{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from IPython.display import display\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "class ApeachDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 split: str,\n",
    "                 tokenizer: Tokenizer, \n",
    "                 max_length: int = 256,\n",
    "                 padding: str = \"max_length\") -> None:\n",
    "        super().__init__()\n",
    "        dataset = load_dataset(\"jason9693/APEACH\")\n",
    "        texts = dataset[split]['text']\n",
    "        inputs = tokenizer(texts, padding=padding, max_length=max_length, truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        self.input_ids = inputs[\"input_ids\"]\n",
    "        self.attention_masks = inputs[\"attention_mask\"]\n",
    "        \n",
    "        labels = dataset[split]['class']\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.input_ids.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index: Any) -> Dict:\n",
    "        return self.input_ids[index], self.attention_masks[index], self.labels[index]\n",
    "\n",
    "    def dataloader(self, **kwargs) -> DataLoader:\n",
    "        return DataLoader(self, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "monologg/koelectra-small-v3-discriminator\n",
    "beomi/KcELECTRA-base\n",
    "beomi/kcbert-base\n",
    "beomi/kcbert-large\n",
    "\"\"\"\n",
    "\n",
    "huggingface_model_name = \"monologg/koelectra-small-v3-discriminator\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(huggingface_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-1815538999999b94\n",
      "Reusing dataset csv (C:\\Users\\heegyukim\\.cache\\huggingface\\datasets\\csv\\default-1815538999999b94\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n",
      "100%|██████████| 2/2 [00:00<00:00, 666.56it/s]\n",
      "Using custom data configuration default-1815538999999b94\n",
      "Reusing dataset csv (C:\\Users\\heegyukim\\.cache\\huggingface\\datasets\\csv\\default-1815538999999b94\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n",
      "100%|██████████| 2/2 [00:00<00:00, 999.60it/s]\n"
     ]
    }
   ],
   "source": [
    "max_length = 64\n",
    "batch_size = 128\n",
    "labels = ['hate']\n",
    "train_dl = ApeachDataset(\"train\", tokenizer, max_length=max_length).dataloader(batch_size=batch_size)\n",
    "val_dl = ApeachDataset(\"test\", tokenizer, max_length=max_length).dataloader(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchmetrics.functional as tm\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from transformers import BertForSequenceClassification, BertConfig\n",
    "\n",
    "\n",
    "def join_step_outputs(outputs):\n",
    "    result = {}\n",
    "    keys = outputs[0].keys()\n",
    "    for k in keys:\n",
    "        X = [x[k] for x in outputs]\n",
    "        if X[0].dim() == 0: # zero-dim tensor\n",
    "            result[k] = torch.stack(X)\n",
    "        else:\n",
    "            result[k] = torch.cat(X, dim=0)\n",
    "    return result\n",
    "\n",
    "class TextClassificationModule(pl.LightningModule):\n",
    "    def __init__(self, huggingface_model_name, labels, lr=5e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(huggingface_model_name, num_labels=len(labels))\n",
    "        # config = {\n",
    "        #     \"max_position_embeddings\": 300,\n",
    "        #     \"hidden_dropout_prob\": 0.1,\n",
    "        #     \"hidden_act\": \"gelu\",\n",
    "        #     \"initializer_range\": 0.02, # 12 to 2\n",
    "        #     \"num_hidden_layers\": 2,\n",
    "        #     \"pooler_num_attention_heads\": 12,\n",
    "        #     \"type_vocab_size\": 2,\n",
    "        #     \"vocab_size\": 30000,\n",
    "        #     \"hidden_size\": 128, # 768 to 128\n",
    "        #     \"attention_probs_dropout_prob\": 0.1,\n",
    "        #     \"num_attention_heads\": 2, # 12 to 2\n",
    "        #     \"intermediate_size\": 512, # 3072 to 512,\n",
    "        #     \"num_labels\": len(labels)\n",
    "        # }\n",
    "        # self.model = BertForSequenceClassification(\n",
    "        #     BertConfig(**config)\n",
    "        # )\n",
    "        self.multiclass = len(labels) > 1\n",
    "        self.criterion = nn.CrossEntropyLoss() if self.multiclass else nn.BCELoss()\n",
    "        self.labels = labels\n",
    "            \n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        logits = self.model(input_ids, attention_mask=attention_mask).logits\n",
    "        if self.multiclass:\n",
    "            logits = logits.softmax(dim=-1)\n",
    "        else:\n",
    "            logits = logits.sigmoid().squeeze(1).float()\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        ids, masks, labels = batch\n",
    "        \n",
    "        logits = self(ids, masks)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        output = {\"loss\": loss, \"logits\": logits, \"labels\": labels}\n",
    "        return output\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        outputs = join_step_outputs(outputs)\n",
    "        loss = outputs[\"loss\"].mean()\n",
    "        self.log(\"train_epoch_loss\", loss)\n",
    "\n",
    "        logits = outputs[\"logits\"]\n",
    "        labels = outputs[\"labels\"]\n",
    "        acc = tm.accuracy(logits, labels.int())\n",
    "        self.log(f\"train_acc\", acc, prog_bar=True)\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        ids, masks, labels = batch\n",
    "        logits = self(ids, masks)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        output = {\"loss\": loss, \"logits\": logits, \"labels\": labels}\n",
    "        return output\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        outputs = join_step_outputs(outputs)\n",
    "        loss = outputs[\"loss\"].mean()\n",
    "        self.log(\"val_epoch_loss\", loss, prog_bar=True)\n",
    "\n",
    "        logits = outputs[\"logits\"]\n",
    "        labels = outputs[\"labels\"]\n",
    "        acc = tm.accuracy(logits, labels.int())\n",
    "        self.log(f\"val_acc\", acc, prog_bar=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-small-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                             | Params\n",
      "---------------------------------------------------------------\n",
      "0 | model     | ElectraForSequenceClassification | 14.1 M\n",
      "1 | criterion | BCELoss                          | 0     \n",
      "---------------------------------------------------------------\n",
      "14.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "14.1 M    Total params\n",
      "56.489    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  99%|█████████▊| 150/152 [02:40<00:02,  1.07s/it, loss=0.291, v_num=120, val_epoch_loss=0.677, val_acc=0.732, train_acc=0.881] \n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "            \n",
    "# val every n steps\n",
    "# https://github.com/PyTorchLightning/pytorch-lightning/issues/2534\n",
    "\n",
    "logger = pl.loggers.TensorBoardLogger(\n",
    "    save_dir='.',\n",
    "    name='lightning_logs'\n",
    ")\n",
    "\n",
    "save_dir = \"./ckpt/\"\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_epoch_loss\",\n",
    "    dirpath=save_dir,\n",
    "    filename=f\"hate_{logger.version}_\" + \"{val_acc:.4f}\",\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "module = TextClassificationModule(\n",
    "    huggingface_model_name,\n",
    "    labels=labels,\n",
    "    lr=1e-4\n",
    ")\n",
    "callbacks = [\n",
    "    EarlyStopping(\"val_epoch_loss\", mode=\"min\", patience=10),\n",
    "    checkpoint_callback\n",
    "]\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=1000, \n",
    "                logger=logger,\n",
    "                gpus=1 if torch.cuda.is_available() else 0,\n",
    "                val_check_interval=20,\n",
    "                callbacks=callbacks)\n",
    "trainer.fit(module, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\heegyukim\\\\Desktop\\\\CurseFilter\\\\ckpt\\\\hate_116_val_acc=0.8122.ckpt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_callback.best_model_path\n",
    "# module = TextClassificationModule.load_from_checkpoint(checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "module.cuda().eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(len(df)):\n",
    "        text = df.text[i]\n",
    "        x = tokenizer(text, return_tensors=\"pt\")\n",
    "        y = module(x['input_ids'].cuda())\n",
    "        preds.append(y.cpu()[0].item())\n",
    "df['preds'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.preds.plot.hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>preds</th>\n",
       "      <th>pred_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>여자가 아이를 돌보고 남자가 밖에서 돈을 벌어온다' 다 옛날 말이죠. 요즘은 다 반...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.623334</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1개월차가 의견 내는 건 안받아 주나봐.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.508775</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>감성 메마른 문과생</td>\n",
       "      <td>0</td>\n",
       "      <td>0.638793</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>같은 성별에 사람이 만난다고 하니 놀라운가봐.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.613181</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>개나 소나 다 대학 갈 필요는 없지. 노래 잘하고 돈 잘벌고 곡도 잘 쓰는데 왜 꼭...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.604390</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3705</th>\n",
       "      <td>채식주의자들은 너무 피곤하게 굴어.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.260329</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3707</th>\n",
       "      <td>남자면 그냥 해도 될텐데 고집을 많이 부리네요. 불편해도 좀 참아야 다들 편한데</td>\n",
       "      <td>1</td>\n",
       "      <td>0.440287</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3719</th>\n",
       "      <td>확실히 서울하고 경상도 사람은 차이가 있어요. 일하는데 드문드문 이상하다 싶으면 여...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.357368</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3729</th>\n",
       "      <td>한국인은 동방예의지국이니까  장유유서를 지키고 차례를 지켜서 밥을 먹는게 당연하지 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.405372</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3764</th>\n",
       "      <td>글 쓰는거 보니까 초딩급이네</td>\n",
       "      <td>1</td>\n",
       "      <td>0.483640</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>756 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  class     preds  \\\n",
       "1     여자가 아이를 돌보고 남자가 밖에서 돈을 벌어온다' 다 옛날 말이죠. 요즘은 다 반...      0  0.623334   \n",
       "5                                1개월차가 의견 내는 건 안받아 주나봐.      0  0.508775   \n",
       "20                                           감성 메마른 문과생      0  0.638793   \n",
       "28                            같은 성별에 사람이 만난다고 하니 놀라운가봐.      0  0.613181   \n",
       "29    개나 소나 다 대학 갈 필요는 없지. 노래 잘하고 돈 잘벌고 곡도 잘 쓰는데 왜 꼭...      0  0.604390   \n",
       "...                                                 ...    ...       ...   \n",
       "3705                                채식주의자들은 너무 피곤하게 굴어.      1  0.260329   \n",
       "3707       남자면 그냥 해도 될텐데 고집을 많이 부리네요. 불편해도 좀 참아야 다들 편한데      1  0.440287   \n",
       "3719  확실히 서울하고 경상도 사람은 차이가 있어요. 일하는데 드문드문 이상하다 싶으면 여...      1  0.357368   \n",
       "3729  한국인은 동방예의지국이니까  장유유서를 지키고 차례를 지켜서 밥을 먹는게 당연하지 ...      1  0.405372   \n",
       "3764                                    글 쓰는거 보니까 초딩급이네      1  0.483640   \n",
       "\n",
       "      pred_class  \n",
       "1              1  \n",
       "5              1  \n",
       "20             1  \n",
       "28             1  \n",
       "29             1  \n",
       "...          ...  \n",
       "3705           0  \n",
       "3707           0  \n",
       "3719           0  \n",
       "3729           0  \n",
       "3764           0  \n",
       "\n",
       "[756 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pred_class'] = (df['preds'] > 0.5).astype(int)\n",
    "error = df[df['class'] != df.pred_class]\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a8f61be024eba58adef938c9aa1e29e02cb3dece83a5348b1a2dafd16a070453"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
